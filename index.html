<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction">
  <meta name="keywords" content="diffusion, nerf">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/grid.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .video-margin {
      margin-bottom: 20px; 
    }
  </style>
  <style>
  .reduce-space {
    margin-bottom: -100px;  
  }
</style>
</head>
<body>


<section class="hero">
  <div class="reduce-space">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class='main-title'>High-Quality Real-Time Rendering <br> Using Subpixel Sampling Reconstruction</span>
          </h1>

            <h2 class="col-md-12 text-center" id="title">
                <b>AAAI 2024</b> <br> 
            </h2>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Boyu Zhang<sup>1, 3</sup>,
            </span>
            <span class="author-block">
              Hongliang Yuan<sup>2, 3</sup>,
            </span>
            <span class="author-block">
              Mingyan Zhu<sup>3, 4</sup>,
            </span>
		<br>
            <span class="author-block">
              Ligang Liu<sup>5</sup>,
            </span>
            <span class="author-block">
              Jue Wang<sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles,</span>
            <span class="author-block"><sup>2</sup>Xiaomi Cooperation,</span>
            <span class="author-block"><sup>3</sup>Tencent AI Lab,</span>
            <span class="author-block"><sup>4</sup>Tsinghua University,</span>
            <span class="author-block"><sup>5</sup>University of Science and Technology of China</span>
            
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">

              <span class="link-block">
                <a href="https://arxiv.org/abs/2301.01036"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://thatbobo.com/SSR.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-video"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

	      <span class="link-block">
                <a href="https://github.com/Luciferbobo/SSR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

	      <span class="link-block">
                <a href="https://github.com/Luciferbobo/SSR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>

            </div>

        </div>
      </div>
    </div>
  </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="rows is-centered">
        <figure>

          <iframe width="100%" height="56.25%" src="static/videos/main.mp4" title="video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


            <h2 class="col-md-12 text-center" id="title">
                <center><b>Our subpixel reconstruction achieves comparable image quality with RAE/NSRR while being 1.5x faster.</center></b> <br> 
            </h2>

          </figure>
        <div class="content has-text-justified">
          <p>
            Generating high-quality, realistic rendering images for real-time applications generally requires tracing a few samples-per-pixel (spp) and using deep learning-based approaches to denoise the resulting low-spp images. Existing denoising methods necessitate a substantial time expenditure when rendering at high resolutions due to the physically-based sampling and network inference time burdens. In this paper, we propose a novel Monte Carlo sampling strategy to accelerate the sampling process and a corresponding denoiser, subpixel sampling reconstruction (SSR), to obtain high-quality images. Extensive experiments demonstrate that our method significantly outperforms previous approaches in denoising quality and reduces overall time costs, enabling real-time rendering capabilities at 2K resolution.
        </p>
        </div>
        <br> 
      </div>
    <!--/ Abstract. -->
  </div>
  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">SSR Method Overview</h2>
      </div>
      <br><br>

      <img src="static/images/overview.png" alt="reconstruction">
      <br><br>
      <p>

Subpixel sampling reconstruction includes two modules: <span style="color: green;">the temporal feature accumulator (TFA)</span> and <span style="color: deepskyblue;">the reconstruction network</span>. TFA consists of two networks, each with two convolution layers that have a spatial support of 3×3 pixels. One network accepts all features and mask of current frame as input and outputs reference embedding. The other computes embeddings for the current features <b>f</b><sub>t</sub> and warped previous features <b>f</b><sub>t-1</sub>. These two embeddings are then pixel-wise multiplied to the reference embedding and then through softmax(·) to get <b>α</b> and <b>β</b> (<b>α</b> + <b>β</b>=1) blending factors for current features and previous features. Our reconstruction network extends U-Net with skip connections, predicts two coarse-scale images at the first two decoder stages, rather than predicting dense features at these stages. Note that all frames shown here are demodulated by albedo.

      </p>      
    </div>
  </div>
  <br><br>

  <div class="container is-max-desktop">
    <div class="rows is-centered">
      <div class="row">
        <h2 class="title is-3 has-text-centered">Comparison to MC denoising approaches</h2>
      </div>

      <br><br>
      
      <div class="comparison-set">
        <p style="text-align: center;">Visual results on scenes BistroInterior, BistroExterior, Sponza, Warmroom and Angel, evaluated at 1024×2048. The ground
truth image is rendered at 32768-spp for reference.</p><br> <!-- Replace with your actual text prompt -->
      <img src="static/images/comparison2.jpg" alt="comparision">
      </div>

      <br><br>


      <div class="row">
        <h2 class="title is-3 has-text-centered">Subpixel MC denoising datasets</h2>
      </div>

      <br><br>

      <div class="comparison-set">
        
      <img src="static/images/datasets.jpg" alt="data">

      </div>
       <br>
      <p> We utilized vulkan-based hybrid ray tracer to generate our subpixel sampling dataset. Our purpose is to optimize our approach for use in 3A gaming and virtual rendering applications. To accomplish this, we opted to conduct separate training for each 3D scene instead of a collective training approach, aligning with the pattern employed by NVIDIA DLSS. The training process was carried out across six distinct scenes, <b>BistroInterior</b>, <b>BistroExterior</b>, <b>Sponza</b>, <b>Diningroom</b>, <b>Angel</b> and <b>Warmroom</b>, contain more than one million triangles and featuring transparency, diffuse, specular, and soft shadow effects. All scenes include 100 to 1000 frames with a resolution of 1024×2048. We also rendered a validation set of 10 frames and a 50 frames test set for each scene. The ground truth image is rendered at 32768-spp for reference.</p>
        </p>
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2023high,
  title={High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction},
  author={Zhang, Boyu and Yuan, Hongliang and Zhu, Mingyan and Liu, Ligang and Wang, Jue},
  journal={arXiv preprint arXiv:2301.01036v2},
  year={2023}
}

or

@article{zhang2023high,
  title={High-Quality Supersampling via Mask-reinforced Deep Learning for Real-time Rendering},
  author={Zhang, Boyu and Yuan, Hongliang and Zhu, Mingyan and Liu, Ligang and Wang, Jue},
  journal={arXiv preprint arXiv:2301.01036v1},
  year={2023}
}
</code></pre>
  </div>
</section>





<footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
	   <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
           </p>
            <p>
              Website template based on <a href="https://github.com/nerfies/nerfies.github.io">this source code</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
